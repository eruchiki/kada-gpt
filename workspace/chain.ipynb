{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67019a11-b40a-486c-9046-adb2b547fae3",
   "metadata": {},
   "source": [
    "# LangChainを試す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80eced48-a494-44cd-b9c7-e6df6d9ed3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.278'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8293113-4ce5-4b32-86d5-e3d41e423e36",
   "metadata": {},
   "source": [
    "## 基本的な生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9dc8e56-a1be-42cf-b437-08ff3a7dc2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.7, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.7)　# temperature(1.0~0.0)が高いとランダム性が高まる\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6fc7fa-3422-497c-b426-714c62751414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「かがわうどん」が香川県で一番おいしいと評判のうどん屋です。\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# LLM ラッパーを初期化\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# LLM に渡す入力テキスト\n",
    "text = \"香川県で一番おいしいうどん屋は？\"\n",
    "\n",
    "# LLM から予測を受け取って表示\n",
    "prediction = llm(text)\n",
    "print(prediction.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e590a-08b6-460c-a034-5fce82b61b6c",
   "metadata": {},
   "source": [
    "## プロンプトテンプレート作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67aed992-1b01-4378-883e-8f7dd5683037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "旅行の情報を共有するためのSNSアプリに名前を付けたいのですがどのような名前がいいですか？\n",
      "・Traveller's Note\n",
      "・TripShare\n",
      "・JourneyLink\n",
      "・TripFolio\n",
      "・Travelingogue\n",
      "・VoyageFlux\n",
      "・RoamingSphere\n",
      "・MyTripLog\n",
      "・VoyageVibe\n",
      "・TripTrack\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# プロンプトテンプレートの作成\n",
    "llm = OpenAI(temperature=0.7)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"app_content\"],\n",
    "    template=\"{app_content}に名前を付けたいのですがどのような名前がいいですか？\",\n",
    ")\n",
    "\n",
    "# テンプレートからプロンプトを作成\n",
    "app_content = \"旅行の情報を共有するためのSNSアプリ\"\n",
    "mid_prompt = prompt.format(app_content=app_content)\n",
    "print(mid_prompt)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "prediction = chain.run(app_content=app_content)\n",
    "print(prediction.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f27986b-18de-4968-b7bc-ad43036056ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。\n",
      "\n",
      "・トラベルコネクト\n",
      "・旅のつながり\n",
      "・旅のシェアー\n",
      "・旅のパートナー\n",
      "・旅のハブ\n",
      "・旅のネットワーク\n",
      "・旅のリンク\n",
      "・旅のブリッジ\n",
      "・旅のコネクション\n",
      "・旅のサークル\n",
      "・旅のサイクル\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 複数変数埋め込みも可\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"app_content\",\"request\"],\n",
    "    template=\"{app_content}に名前を付けたいのですがどのような名前がいいですか？{request}\",\n",
    ")\n",
    "\n",
    "# テンプレートからプロンプトを作成\n",
    "app_content = \"旅行の情報を共有するためのSNSアプリ\"\n",
    "request = \"日本語でお願いします\"\n",
    "mid_prompt = prompt.format(app_content=app_content,request=request)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "prediction = chain.run(app_content=app_content,request=request)\n",
    "print(prediction.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340bf373-d9d8-4bc5-8171-a188631e918f",
   "metadata": {},
   "source": [
    "## エージェントを用いる\n",
    "ツール一覧\n",
    "- Serpapi…SerpAPIを使用した検索エンジン\n",
    "- Request(API連携)…特定のサイトからコンテンツを取得するツール\n",
    "- Terminal…コマンド実行\n",
    "- Google Serper…検索エンジンがGoogle\n",
    "- Wikipedia…Wikipedia検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8f932-c045-4c6e-91ed-f3f478d53de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# ツールを指定\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "#エージェントにタスクを実行してもらいます\n",
    "agent.run(\"昨日の東京の最高気温は何度でしたか？摂氏温度でお答え下さい。また、その数値を x としたとき、x^0.23 は何ですか？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167c148-2d59-428c-99d1-bf42ac35dcc9",
   "metadata": {},
   "source": [
    "## 以前の入出力を記憶させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "456252f2-b19b-499d-ad5d-b8b86684c654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  こんにちは。最近オランダに行きました\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: 。\n",
      "\n",
      "おお、おめでとうございます！オランダはどんなところでしたか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  きれいな町でした\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: It was a beautiful town.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  僕が最近どこに行ったか覚えていますか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: はい、覚えています。最近、私は東京都内のいくつかの場所に行きました。その中で、最も有名なのは、東京タワーと東京スカイツリーです。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "while True: \n",
    "    command = input(\"You: \")\n",
    "    if command == \"exit\":\n",
    "        break\n",
    "    response = llm(command)\n",
    "    print(f\"AI: {response.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ca3977-088b-43aa-a9db-25581c506890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  こんにちは。最近オランダに行きました\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  こんにちは！オランダに行ったんですね！どんなところですか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  きれいな町でしたよ。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  そうなんですか！オランダは有名なチェスセンターでもあるんですね。どんなチェスセンターを見ましたか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  チェスセンターにはいってません。そういえばオランダの首都は知ってますか？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  はい、オランダの首都はアムステルダムです。アムステルダムは、歴史的な建物や文化的な場所がたくさんあるとても美しい町です。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "# 前回のやり取りを記憶する\n",
    "conversation = ConversationChain(llm=llm)\n",
    "while True: \n",
    "    command = input(\"You: \")\n",
    "    if command == \"exit\":\n",
    "        break\n",
    "    response = conversation.predict(input=command)\n",
    "    print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ee93e-59a2-404c-b436-60e89b4e6e00",
   "metadata": {},
   "source": [
    "## 対話に設定入れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6d4ad-4e66-456a-9c03-fd2676bfdb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "# チャットモデルで利用可能なメッセージの型をインポート\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "# チャットモデルのラッパーを初期化\n",
    "chat = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "# チャットモデルに渡すメッセージを作成する\n",
    "messages = [\n",
    "    SystemMessage(content=\"あなたは親切なアシスタントです。\"),\n",
    "    HumanMessage(content=\"春の季語を絡めた冗談を教えてください。\"),\n",
    "    AIMessage(content=\"「春眠（しゅんみん）暁（ぎょう）を覚（さ）えず」という言葉がありますが、「春は眠くても、アシスタントは覚えてるよ！」と言って、ツッコミを入れるのはいかがでしょうか？笑\"),\n",
    "    HumanMessage(content=\"面白くない。もう一度。\"),\n",
    "]\n",
    "\n",
    "# チャットモデルにメッセージを渡して、予測を受け取る\n",
    "result = chat(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90065d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 8, 9, 10, 9, 10, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\",\"\"],  \n",
    "    chunk_size=10,  # チャンクのトークン数\n",
    "    chunk_overlap=0,\n",
    "    length_function = len,  # チャンクオーバーラップのトークン数\n",
    ")\n",
    "text = \"あ\\nあああああ\\nああ\\nあああああ\\nああ\\nあああああああああああああああああああ\\nああああああああああああああああああああ\"\n",
    "result = text_splitter.split_text(text)\n",
    "[len(enc.encode(r)) for r in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd1e8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"メトリックの統合によるツイート話題分類の高精度化に関する検討AStudyonImprovingPerformanceofTweetTopicClassificationbyIntegratingMetrics熊本忠彦†TadahikoKumamoto†1.はじめに近年,情報通信インフラの発展と普及に伴い,SocialNetworkingService(SNS)を利用する人が増えている[1].特に,マイクロブログの一つであるTwitter[2]は,他のSNSと比べ,一度に投稿できる文字数が140字までと比較的少なく,自分の本当のプロフィールを明らかにする必要もないことから,年齢や性別,職業といった立場を考えずに気軽に投稿できるという特徴がある.そのため,Twitterでは,日々の身近なことから社会全体に対することまで様々な話題に関して多種多様な意見やコメント等が発信されている.様々な話題の中から特定の話題に関するツイートを集め,より粒度の細かい話題に分類することで,その話題に関し,人々がどのようなことを感じ,発信しているのかを分析することが可能となる.例えば,2020年以降に起きた新型コロナウイルス感染症の世界的流行では,不要不急な行動や三密(密閉・密集・密接)が制限され,抑圧された社会生活を過ごさなくてはならなくなったこともあり,新型コロナウイルス感染症やコロナ禍の現状などに関する不平や不満,想い,考えが数多くツイートされている.こういったツイートに話題分類を適用することで,コロナ禍に関する多くの人々の本音を分析することが可能となり,QOL(生活の質;QualityOfLife)の向上に資するものと考えられる.そこで本稿では,コロナ禍に関連するツイートを対象に,より粒度の細かい話題に分類する深層学習ベースの手法[3]を示し,その高精度化について検討する.具体的には,ツイートとそのツイートから抽出されるトピックをベクトル化し,両者を比較することで,より高精度な話題分類を実現する.トピックの抽出には,代表的なトピックモデルの一つであるLatentDirichletAllocation(LDA)[4]を用い,ツイートやトピックのベクトル化には,深層学習ベースの分散表現生成手法の一つであるfastText[5]を用いる.なお,ベクトルどうしの近さを測るためのメトリックとして\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d29f8ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec66422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['あ\\nあああああ\\nああ',\n",
       " 'あああああ\\nああ',\n",
       " 'あああああああああ',\n",
       " 'ああああああああああ',\n",
       " 'あああああああああ',\n",
       " 'ああああああああああ',\n",
       " 'あ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cf408fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'あ\\nいう\\nえ\\nおお'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "a = \"。.．\"\n",
    "b =\"あ。いう.え．おお\"\n",
    "s = \"[\"+a+\"]\"\n",
    "re.sub(s, '\\n', b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
