{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca51c9d-a725-4258-bf75-aa9fa5fe8938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import uuid\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "COLLECTION_NAME = \"DocumentsDB_Langchain\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llm=OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8410d-999c-44ef-9462-b23bf2c5d5d4",
   "metadata": {},
   "source": [
    "## Qdrantに接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd96938-4786-4635-b064-affa02243ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful: <qdrant_client.qdrant_client.QdrantClient object at 0x7fe3ddf10a90>\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient,models\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "client = QdrantClient(host='qdrant', port=6333)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "print(\"Connection successful:\", client)\n",
    "\n",
    "db = Qdrant(\n",
    "    client=client, collection_name=COLLECTION_NAME, \n",
    "    embeddings=embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49d6b0-7108-479b-83bd-5c1511f267e9",
   "metadata": {},
   "source": [
    "## 関連情報探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a8aa4f0-91fd-4232-a734-9fc9ea7e0c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(注)業務の遂行上必要な範囲での利用には、光ネットサービス契約者に係る情報を当社の業務を委託している者に提供する場合を含みます。\n",
      "4当社が別に定める光ネットサービス当社は、下記の契約約款に定める光ネットサービスを当社が別に定める光ネットサービスとして扱います。\n",
      "2前項の確認に際して、光ネットサービス契約者から請求があったときは、当社は、光ネットサービス取扱局において試験を行い、その結果を光ネットサービス契約者に通知します。\n",
      "3光ネットサービス契約者は、前2項に規定する接続について、第1項の書面に記載した事項を変更しようとするときは、当社所定の書面によりその変更の請求をしていただきます。この場合、当社は、前項の規定に準じて取り扱います。\n",
      "第1章総則(約款の適用)第1条当社は、このビジネス光ネットサービス契約約款(料金表を含みます。以下「約款」といいます。)を定め、これにより光ネットサービス(当社がこの約款以外の契約約款及び料金表を定め、それにより提供するものを除きます。)を提供します。\n",
      "2当社は、当社が指定する光ネットサービス取扱所において、光ネットサービスを利用する上で参考となる、別記13に定める事項を記載した技術資料を閲覧に供します。\n",
      "(品目及び種別等の変更)第14条光ネットサービス契約者は、当社に対し、当社が別に定めるところにより光ネットサービスの品目及び種別等の変更を請求することができます。\n",
      "4光ネットサービス契約者は、第1項及び第2項に規定する接続を廃止しようとするときは、あらかじめ書面により光ネットサービス取扱所に通知していただきます。\n",
      "区分内容(1)事務手数料等に係る料金の適用ア光ネットサービス契約の申込みをし、その承諾を受けたときに契約事務手数料を適用します。\n",
      "2前項の場合において、当社は譲り受けた債権を当社が提供する光ネットサービスの料金とみなして取り扱います。\n"
     ]
    }
   ],
   "source": [
    "query = \"ビジネス光ネットサービスの契約概要について教えて\"\n",
    "docs = db.similarity_search(query=query,k=10)\n",
    "for d in docs:\n",
    "    print(d.page_content)\n",
    "# related_info = retriever.get_relevant_documents(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relate_search(k=3,filter=None):\n",
    "    docs = db.similarity_search(query=query,k=10,filter=None)\n",
    "    return [d.page_content for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a3c6354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "3 4\n"
     ]
    }
   ],
   "source": [
    "a = [(1,2),(3,4)]\n",
    "for i,j in a:\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c64369-d7d5-498f-8411-175bcb1024a1",
   "metadata": {},
   "source": [
    "## memoryの実装\n",
    "- ConversationBufferWindowMemory  \n",
    " 直前数回のやり取りをプロンプトに入れる\n",
    "- VectorStore-Backed Memory  \n",
    "  プロンプトをベクトル化し、近いやり取りを埋め込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90cd7f37-b198-443b-a314-fd277850c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory,VectorStoreRetrieverMemory\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f82746-31df-45ae-abf3-a6f05ae677dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ConversationBufferWindowMemory(k=3)\n",
    "\n",
    "def save_memory(memory,query,response):\n",
    "    memory.chat_memory.add_user_message(query)\n",
    "    memory.chat_memory.add_ai_message(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235eab91-0639-4dae-883c-b043424d7315",
   "metadata": {},
   "source": [
    "## promptTemplate作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f0657-b0c4-4156-8b22-97eeb3fd30e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{chat_history}\n",
    "質問：{query}以下の関連情報をもとに答えてください。\n",
    "関連情報\n",
    "1.{relate_info1}\n",
    "2.{relate_info2}\n",
    "3.{relate_info3}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e54123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "質問：a\n",
      "関連情報\n",
      "1.b\n",
      "2.b\n",
      "解答：\n"
     ]
    }
   ],
   "source": [
    "def create_template(relate_info):\n",
    "    input_val = [\"chat_history\",\"query\"]\n",
    "    template = \"{chat_history}\\n質問：{query}\\n関連情報\\n\"\n",
    "    for i,info in enumerate(relate_info):\n",
    "        template += f\"{i+1}.{info}\\n\"\n",
    "    template += \"解答：\"\n",
    "    return PromptTemplate(input_variables=input_val,template=template)\n",
    "\n",
    "template = create_template([\"b\",\"b\"])\n",
    "prompt = template.format(chat_history=\"\",query=\"a\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5493f4",
   "metadata": {},
   "source": [
    "## 対話"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c3b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(query,t):\n",
    "    related_data = relate_search(query,k=10)\n",
    "    template = create_template(related_data)\n",
    "    memory = ConversationBufferWindowMemory(memory_key=\"chat_history\",k=3)\n",
    "    memory.chat_memory.add_user_message(\"hi!\")\n",
    "    memory.chat_memory.add_ai_message(\"what's up?\")\n",
    "    llm_chain = LLMChain(\n",
    "    llm=OpenAI(),\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    )\n",
    "    return llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39b1c648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: 今から日にちを言うのでその日の曜日を答えてください\n",
      "AI: 分かりました\n",
      "質問：9月25日は？\n",
      "関連情報\n",
      "1.9月24日は日曜日です\n",
      "解答：\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "月曜日\n",
      "<class 'langchain.callbacks.openai_info.OpenAICallbackHandler'>\n",
      "Tokens Used: 115\n",
      "\tPrompt Tokens: 108\n",
      "\tCompletion Tokens: 7\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0023\n",
      "{'successful_requests': 1, 'total_cost': 0.0023, 'total_tokens': 115, 'prompt_tokens': 108, 'completion_tokens': 7}\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "template = create_template([\"9月24日は日曜日です\"])\n",
    "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\",k=3)\n",
    "memory.chat_memory.add_user_message(\"今から日にちを言うのでその日の曜日を答えてください\")\n",
    "memory.chat_memory.add_ai_message(\"分かりました\")\n",
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(),\n",
    "    prompt=template,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    )\n",
    "with get_openai_callback() as cb:\n",
    "    response = llm_chain(\"9月25日は？\")\n",
    "print(response[\"text\"])\n",
    "print(type(cb))\n",
    "print(cb)\n",
    "print(cb.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e2afb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__config__': pydantic.v1.config.Config,\n",
       "              '__fields__': {'memory': ModelField(name='memory', type=Optional[BaseMemory], required=False, default=None),\n",
       "               'callbacks': ModelField(name='callbacks', type=Union[List[langchain.callbacks.base.BaseCallbackHandler], BaseCallbackManager, NoneType], required=False, default=None),\n",
       "               'callback_manager': ModelField(name='callback_manager', type=Optional[BaseCallbackManager], required=False, default=None),\n",
       "               'verbose': ModelField(name='verbose', type=bool, required=False, default_factory='<function _get_verbosity>'),\n",
       "               'tags': ModelField(name='tags', type=Optional[List[str]], required=False, default=None),\n",
       "               'metadata': ModelField(name='metadata', type=Optional[Mapping[str, Any]], required=False, default=None),\n",
       "               'prompt': ModelField(name='prompt', type=BasePromptTemplate, required=True),\n",
       "               'llm': ModelField(name='llm', type=BaseLanguageModel, required=True),\n",
       "               'output_key': ModelField(name='output_key', type=str, required=False, default='text'),\n",
       "               'output_parser': ModelField(name='output_parser', type=BaseLLMOutputParser, required=False, default_factory='<function StrOutputParser>'),\n",
       "               'return_final_only': ModelField(name='return_final_only', type=bool, required=False, default=True),\n",
       "               'llm_kwargs': ModelField(name='llm_kwargs', type=dict, required=False, default_factory='<function dict>')},\n",
       "              '__exclude_fields__': {'callbacks': True,\n",
       "               'callback_manager': True},\n",
       "              '__include_fields__': None,\n",
       "              '__validators__': {'verbose': [<pydantic.v1.class_validators.Validator at 0x7fe3e3de60c0>]},\n",
       "              '__pre_root_validators__': [],\n",
       "              '__post_root_validators__': [(False,\n",
       "                <function langchain.chains.base.Chain.raise_callback_manager_deprecation(cls, values: Dict) -> Dict>)],\n",
       "              '__schema_cache__': {},\n",
       "              '__json_encoder__': <staticmethod(<function pydantic_encoder at 0x7fe3febe36a0>)>,\n",
       "              '__custom_root_type__': False,\n",
       "              '__private_attributes__': {'_lc_kwargs': ModelPrivateAttr(default=PydanticUndefined, default_factory=<class 'dict'>)},\n",
       "              '__slots__': set(),\n",
       "              '__hash__': None,\n",
       "              '__class_vars__': set(),\n",
       "              '__module__': 'langchain.chains.llm',\n",
       "              '__annotations__': {'prompt': 'BasePromptTemplate',\n",
       "               'llm': 'BaseLanguageModel',\n",
       "               'output_key': 'str',\n",
       "               'output_parser': 'BaseLLMOutputParser',\n",
       "               'return_final_only': 'bool',\n",
       "               'llm_kwargs': 'dict'},\n",
       "              '__doc__': 'Chain to run queries against LLMs.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import LLMChain\\n            from langchain.llms import OpenAI\\n            from langchain.prompts import PromptTemplate\\n            prompt_template = \"Tell me a {adjective} joke\"\\n            prompt = PromptTemplate(\\n                input_variables=[\"adjective\"], template=prompt_template\\n            )\\n            llm = LLMChain(llm=OpenAI(), prompt=prompt)\\n    ',\n",
       "              'lc_serializable': <property at 0x7fe3e3de4310>,\n",
       "              'Config': langchain.chains.llm.LLMChain.Config,\n",
       "              'input_keys': <property at 0x7fe3e3de6660>,\n",
       "              'output_keys': <property at 0x7fe3e3de5300>,\n",
       "              '_call': <function langchain.chains.llm.LLMChain._call(self, inputs: 'Dict[str, Any]', run_manager: 'Optional[CallbackManagerForChainRun]' = None) -> 'Dict[str, str]'>,\n",
       "              'generate': <function langchain.chains.llm.LLMChain.generate(self, input_list: 'List[Dict[str, Any]]', run_manager: 'Optional[CallbackManagerForChainRun]' = None) -> 'LLMResult'>,\n",
       "              'agenerate': <function langchain.chains.llm.LLMChain.agenerate(self, input_list: 'List[Dict[str, Any]]', run_manager: 'Optional[AsyncCallbackManagerForChainRun]' = None) -> 'LLMResult'>,\n",
       "              'prep_prompts': <function langchain.chains.llm.LLMChain.prep_prompts(self, input_list: 'List[Dict[str, Any]]', run_manager: 'Optional[CallbackManagerForChainRun]' = None) -> 'Tuple[List[PromptValue], Optional[List[str]]]'>,\n",
       "              'aprep_prompts': <function langchain.chains.llm.LLMChain.aprep_prompts(self, input_list: 'List[Dict[str, Any]]', run_manager: 'Optional[AsyncCallbackManagerForChainRun]' = None) -> 'Tuple[List[PromptValue], Optional[List[str]]]'>,\n",
       "              'apply': <function langchain.chains.llm.LLMChain.apply(self, input_list: 'List[Dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'List[Dict[str, str]]'>,\n",
       "              'aapply': <function langchain.chains.llm.LLMChain.aapply(self, input_list: 'List[Dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'List[Dict[str, str]]'>,\n",
       "              '_run_output_key': <property at 0x7fe3e3de6610>,\n",
       "              'create_outputs': <function langchain.chains.llm.LLMChain.create_outputs(self, llm_result: 'LLMResult') -> 'List[Dict[str, Any]]'>,\n",
       "              '_acall': <function langchain.chains.llm.LLMChain._acall(self, inputs: 'Dict[str, Any]', run_manager: 'Optional[AsyncCallbackManagerForChainRun]' = None) -> 'Dict[str, str]'>,\n",
       "              'predict': <function langchain.chains.llm.LLMChain.predict(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'str'>,\n",
       "              'apredict': <function langchain.chains.llm.LLMChain.apredict(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'str'>,\n",
       "              'predict_and_parse': <function langchain.chains.llm.LLMChain.predict_and_parse(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[str, List[str], Dict[str, Any]]'>,\n",
       "              'apredict_and_parse': <function langchain.chains.llm.LLMChain.apredict_and_parse(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[str, List[str], Dict[str, str]]'>,\n",
       "              'apply_and_parse': <function langchain.chains.llm.LLMChain.apply_and_parse(self, input_list: 'List[Dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'Sequence[Union[str, List[str], Dict[str, str]]]'>,\n",
       "              '_parse_generation': <function langchain.chains.llm.LLMChain._parse_generation(self, generation: 'List[Dict[str, str]]') -> 'Sequence[Union[str, List[str], Dict[str, str]]]'>,\n",
       "              'aapply_and_parse': <function langchain.chains.llm.LLMChain.aapply_and_parse(self, input_list: 'List[Dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'Sequence[Union[str, List[str], Dict[str, str]]]'>,\n",
       "              '_chain_type': <property at 0x7fe3e3de66b0>,\n",
       "              'from_string': <classmethod(<function LLMChain.from_string at 0x7fe3e3dee200>)>,\n",
       "              '__parameters__': (),\n",
       "              '__abstractmethods__': frozenset(),\n",
       "              '_abc_impl': <_abc._abc_data at 0x7fe3e3df2580>,\n",
       "              '__signature__': <pydantic.v1.utils.ClassAttribute at 0x7fe3e3fc7970>})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLMChain.__dict__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
