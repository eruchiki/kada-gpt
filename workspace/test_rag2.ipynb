{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b1c1ef-dc41-426f-bff3-cf72e4c15f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import download_loader\n",
    "\n",
    "CJKPDFReader = download_loader(\"CJKPDFReader\")\n",
    "loader = CJKPDFReader()\n",
    "documents = loader.load_data(file=\"pdf_data/data_2.pdf\")\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4512fcbb-5083-4aba-b484-b660fed6b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./pdf_to_text/pdf_to_text_2.txt\",encoding=\"utf-8\",mode=\"w\") as f:\n",
    "    f.write(documents[0].text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9bf3d9d-dcec-4847-b6e4-5e42d96e9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_separator = \"\\n\\n\" # 段落分割\n",
    "chunk_size = 1024 #チャンク（トークン）数\n",
    "chunk_overlap = 20 # 前のチャンクをどのくらい含めるか\n",
    "secondary_chunking_regex = '[^,.．;。]+[,.．;。]?'# 文分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84eb8b70-a310-4de4-bc13-4298d906777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import VectorStoreIndex,ServiceContext, LLMPredictor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# ノードパーサーの準備\n",
    "text_splitter = SentenceSplitter(\n",
    "    paragraph_separator=paragraph_separator,\n",
    "    secondary_chunking_regex = secondary_chunking_regex,\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap,\n",
    ")\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    text_splitter=text_splitter\n",
    ")\n",
    "\n",
    "llm_predictor_gpt3 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "llm_predictor_gpt4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\n",
    "\n",
    "# embedding_llm = LangchainEmbedding(\n",
    "#     OpenAIEmbeddings(\n",
    "#         model=embedding_llm_model_name,\n",
    "#         deployment=embedding_llm_deployment_name,\n",
    "#         openai_api_key= openai.api_key,\n",
    "#         openai_api_base=openai.api_base,\n",
    "#         openai_api_type=openai.api_type,\n",
    "#         openai_api_version=openai.api_version,\n",
    "#     ),\n",
    "#     embed_batch_size=1,\n",
    "# )\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    node_parser=node_parser,\n",
    "    # embed_model=embedding_llm,\n",
    "    llm_predictor=llm_predictor_gpt3\n",
    ")\n",
    "\n",
    "service_context_gpt4 = ServiceContext.from_defaults(\n",
    "    node_parser=node_parser,\n",
    "    # embed_model=embedding_llm,\n",
    "    llm_predictor=llm_predictor_gpt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "602e6d9c-3d9c-4238-a994-277d59602313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful: <qdrant_client.qdrant_client.QdrantClient object at 0x7ff4106007d0>\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Qdrantクライアントを用意\n",
    "client = QdrantClient(host='qdrant', port=6333)\n",
    "print(\"Connection successful:\", client)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89f255a-5d3c-4b2f-ad0c-05028af782ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://qdrant:6333/collections/test_rag3 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://qdrant:6333/collections/test_rag3 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://qdrant:6333/collections/test_rag3 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT http://qdrant:6333/collections/test_rag3/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT http://qdrant:6333/collections/test_rag3/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT http://qdrant:6333/collections/test_rag3/points?wait=true \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# VectorStoreIndexをdocumentsから構築\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test_rag3\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cd1209e-be20-4c1f-bcc9-010dd7461770",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "# gpt4を使う場合\n",
    "# query_engine = index.as_query_engine(service_context=service_context_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffd67792-d844-4167-9f2b-b4ba7f89641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://qdrant:6333/collections/test_rag3/points/search \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://qdrant:6333/collections/test_rag3/points/search \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://qdrant:6333/collections/test_rag3/points/search \"HTTP/1.1 200 OK\"\n",
      "提案システムでは、学習者の進捗や理解度に合わせたヒントを提示するために、学習者と演習用サーバの間にエージェントを配置しています。エージェントは学習者の行き詰まりを検知し、その原因を推測してヒントを提示します。ヒントは事前に教授者が演習用サーバ内のデータベースに用意しておきます。演習用サーバは学習者からのヒント要求を受け取り、データベースからヒントを取得してエージェントに渡します。エージェントは受け取ったヒントを学習者に提示します。このようにして、学習者が教授者の手を借りずに演習を進めることができるシステムを提案しています。\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"結局どういう実装をしているんですか？日本語で答えてください．\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb24986-b356-4227-bd41-87c19377794c",
   "metadata": {},
   "source": [
    "# ベクターストアからの復元(失敗)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e111c02-7383-459c-99fd-d9a5a6623d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT http://qdrant:6333/collections/test_rag2/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT http://qdrant:6333/collections/test_rag2/points?wait=true \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "vector_store2 = QdrantVectorStore(client=client, collection_name=\"test_rag2\")\n",
    "storage_context2 = StorageContext.from_defaults(vector_store=vector_store2)\n",
    "index2 = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context2, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc68b902-1e70-4654-be84-68f24213762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine2 = index2.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13fa191b-04d8-409c-8e5e-3e26a5cfc8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://qdrant:6333/collections/test_rag2/points/search \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://qdrant:6333/collections/test_rag2/points/search \"HTTP/1.1 200 OK\"\n",
      "提案手法は、画像加工による異常検知モデル学習用データセットの圧縮手法です。まず、各画像の切り抜き領域を示すマスク画像の論理和を求め、すべてのオブジェクトが収まる最小領域を持ったマスク画像を作成します。そして、このマスク画像を使用して、全ての画像に対してマスク処理を行い、オブジェクトに干渉しない範囲の背景を大まかに削除します。その後、加工した画像データセットをHEVCの非可逆圧縮モードを用いて動画ファイルに変換します。HEVCによる変換では、画像間の冗長性を排除して符号化されるため、画像データセットサイズの大幅な縮小が期待されます。最後に、モデルの学習時には、一度動画ファイルへと変換したデータセットを再び個々の画像ファイルに復元し、従来と同様の方法で学習処理を行います。\n"
     ]
    }
   ],
   "source": [
    "response2 = query_engine2.query(\"提案手法の概要を説明してください\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c0b66-c3c5-4164-a0a5-db4cd625d231",
   "metadata": {},
   "source": [
    "# ベクターストアから復元(成功？)\n",
    "一応，embeddingは質問の1回だけしか行われなかった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c4d51d-30fc-4420-99b3-4b88ac798776",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mindex2\u001b[49m\u001b[38;5;241m.\u001b[39mstorage_context\u001b[38;5;241m.\u001b[39mpersist(persist_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./storage_context\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# documentもqdrantに保存されていて，出力されるjsonはそのアドレスを示しているだけ？\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index2' is not defined"
     ]
    }
   ],
   "source": [
    "index2.storage_context.persist(persist_dir=\"./storage_context\")\n",
    "# documentもqdrantに保存されていて，出力されるjsonはそのアドレスを示しているだけ？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98763aac-673d-4e93-8a83-089dc56b93a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful: <qdrant_client.qdrant_client.QdrantClient object at 0x7ff449bb6550>\n",
      "INFO:httpx:HTTP Request: GET http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import VectorStoreIndex,ServiceContext, LLMPredictor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "import logging\n",
    "import sys\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index import load_index_from_storage\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import VectorStoreIndex,ServiceContext, LLMPredictor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Qdrantクライアントを用意\n",
    "client = QdrantClient(host='qdrant', port=6333)\n",
    "print(\"Connection successful:\", client)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "vector_store3 = QdrantVectorStore(client=client, collection_name=\"test_rag2\")\n",
    "\n",
    "storage_context3 = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"./storage_context\"),\n",
    "    vector_store=vector_store3,\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"./storage_context\"),\n",
    ")\n",
    "\n",
    "paragraph_separator = \"\\n\\n\" # 段落分割\n",
    "chunk_size = 1024 #チャンク（トークン）数\n",
    "chunk_overlap = 20 # 前のチャンクをどのくらい含めるか\n",
    "secondary_chunking_regex = '[^,.．;。]+[,.．;。]?'# 文分割\n",
    "\n",
    "# ノードパーサーの準備\n",
    "text_splitter = SentenceSplitter(\n",
    "    paragraph_separator=paragraph_separator,\n",
    "    secondary_chunking_regex = secondary_chunking_regex,\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap,\n",
    ")\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    text_splitter=text_splitter\n",
    ")\n",
    "\n",
    "llm_predictor_gpt3 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "llm_predictor_gpt4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\n",
    "\n",
    "# embedding_llm = LangchainEmbedding(\n",
    "#     OpenAIEmbeddings(\n",
    "#         model=embedding_llm_model_name,\n",
    "#         deployment=embedding_llm_deployment_name,\n",
    "#         openai_api_key= openai.api_key,\n",
    "#         openai_api_base=openai.api_base,\n",
    "#         openai_api_type=openai.api_type,\n",
    "#         openai_api_version=openai.api_version,\n",
    "#     ),\n",
    "#     embed_batch_size=1,\n",
    "# )\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    node_parser=node_parser,\n",
    "    # embed_model=embedding_llm,\n",
    "    llm_predictor=llm_predictor_gpt3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91077b46-335f-4d6a-a0fb-ceb035953c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "index3 = load_index_from_storage(storage_context3, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43073b40-1aab-4e88-bba2-cdacf05d3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine3 = index3.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6b1380-02fa-4d2f-b54e-f53fb779965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://qdrant:6333/collections/test_rag2/points/search \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://qdrant:6333/collections/test_rag2/points/search \"HTTP/1.1 200 OK\"\n",
      "画像は、異常検知モデルの学習用データセットの一部として使用されるものを想定しています。\n"
     ]
    }
   ],
   "source": [
    "response3 = query_engine3.query(\"画像はどのようなものを想定していますか？日本語で答えてください．\")\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d327ca-97e8-4401-add8-8637bef83e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
