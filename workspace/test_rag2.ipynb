{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b1c1ef-dc41-426f-bff3-cf72e4c15f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import download_loader\n",
    "\n",
    "CJKPDFReader = download_loader(\"CJKPDFReader\")\n",
    "loader = CJKPDFReader()\n",
    "documents = loader.load_data(file=\"pdf_data/data_1.pdf\")\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4512fcbb-5083-4aba-b484-b660fed6b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./pdf_to_text/pdf_to_text_1.txt\",encoding=\"utf-8\",mode=\"w\") as f:\n",
    "    f.write(documents[0].text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9bf3d9d-dcec-4847-b6e4-5e42d96e9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_separator = \"\\n\\n\" # 段落分割\n",
    "chunk_size = 1024 #チャンク（トークン）数\n",
    "chunk_overlap = 20 # 前のチャンクをどのくらい含めるか\n",
    "secondary_chunking_regex = '[^,.．;。]+[,.．;。]?'# 文分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84eb8b70-a310-4de4-bc13-4298d906777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import VectorStoreIndex,ServiceContext, LLMPredictor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# ノードパーサーの準備\n",
    "text_splitter = SentenceSplitter(\n",
    "    paragraph_separator=paragraph_separator,\n",
    "    secondary_chunking_regex = secondary_chunking_regex,\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap,\n",
    ")\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    text_splitter=text_splitter\n",
    ")\n",
    "\n",
    "llm_predictor_gpt3 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "llm_predictor_gpt4 = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"))\n",
    "\n",
    "# embedding_llm = LangchainEmbedding(\n",
    "#     OpenAIEmbeddings(\n",
    "#         model=embedding_llm_model_name,\n",
    "#         deployment=embedding_llm_deployment_name,\n",
    "#         openai_api_key= openai.api_key,\n",
    "#         openai_api_base=openai.api_base,\n",
    "#         openai_api_type=openai.api_type,\n",
    "#         openai_api_version=openai.api_version,\n",
    "#     ),\n",
    "#     embed_batch_size=1,\n",
    "# )\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    node_parser=node_parser,\n",
    "    # embed_model=embedding_llm,\n",
    "    llm_predictor=llm_predictor_gpt3\n",
    ")\n",
    "\n",
    "service_context_gpt4 = ServiceContext.from_defaults(\n",
    "    node_parser=node_parser,\n",
    "    # embed_model=embedding_llm,\n",
    "    llm_predictor=llm_predictor_gpt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "602e6d9c-3d9c-4238-a994-277d59602313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful: <qdrant_client.qdrant_client.QdrantClient object at 0x7efcbf7dd690>\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Qdrantクライアントを用意\n",
    "client = QdrantClient(host='qdrant', port=6333)\n",
    "print(\"Connection successful:\", client)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b89f255a-5d3c-4b2f-ad0c-05028af782ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 404 Not Found\"\n",
      "HTTP Request: GET http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 404 Not Found\"\n",
      "INFO:httpx:HTTP Request: DELETE http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: DELETE http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT http://qdrant:6333/collections/test_rag2/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT http://qdrant:6333/collections/test_rag2/points?wait=true \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# VectorStoreIndexをdocumentsから構築\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test_rag2\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd1209e-be20-4c1f-bcc9-010dd7461770",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "# gpt4を使う場合\n",
    "# query_engine = index.as_query_engine(service_context=service_context_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd67792-d844-4167-9f2b-b4ba7f89641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://qdrant:6333/collections/test_rag2/points/search \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://qdrant:6333/collections/test_rag2/points/search \"HTTP/1.1 200 OK\"\n",
      "提案手法は、画像加工を組み合わせることによって、異常検知精度の維持と高い圧縮率の両立を実現する手法です。具体的には、各画像の切り抜き領域を示すマスク画像を作成し、オブジェクトに干渉しない範囲の背景を大まかに削除します。その後、加工した画像データセットを動画ファイルに変換し、HEVCの非可逆圧縮モードを用いて圧縮します。HEVCによる変換では、画像間の冗長性を排除して符号化されるため、画像データセットのサイズが大幅に縮小されます。そして、モデルの学習時には、動画ファイルから個々の画像ファイルに復元して学習処理を行います。このように、提案手法は画像加工と動画ファイルへの変換を組み合わせることで、異常検知モデルの学習用データセットを効果的に圧縮することができます。\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"提案手法の概要を説明してください\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb24986-b356-4227-bd41-87c19377794c",
   "metadata": {},
   "source": [
    "# ベクターストアからの復元(失敗)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e111c02-7383-459c-99fd-d9a5a6623d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://qdrant:6333/collections/test_rag2 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT http://qdrant:6333/collections/test_rag2/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: PUT http://qdrant:6333/collections/test_rag2/points?wait=true \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "vector_store2 = QdrantVectorStore(client=client, collection_name=\"test_rag2\")\n",
    "storage_context2 = StorageContext.from_defaults(vector_store=vector_store2)\n",
    "index2 = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context2, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc68b902-1e70-4654-be84-68f24213762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine2 = index2.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13fa191b-04d8-409c-8e5e-3e26a5cfc8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://qdrant:6333/collections/test_rag2/points/search \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://qdrant:6333/collections/test_rag2/points/search \"HTTP/1.1 200 OK\"\n",
      "提案手法は、画像加工による異常検知モデル学習用データセットの圧縮手法です。まず、各画像の切り抜き領域を示すマスク画像の論理和を求め、すべてのオブジェクトが収まる最小領域を持ったマスク画像を作成します。そして、このマスク画像を使用して、全ての画像に対してマスク処理を行い、オブジェクトに干渉しない範囲の背景を大まかに削除します。その後、加工した画像データセットをHEVCの非可逆圧縮モードを用いて動画ファイルに変換します。HEVCによる変換では、画像間の冗長性を排除して符号化されるため、画像データセットサイズの大幅な縮小が期待されます。最後に、モデルの学習時には、一度動画ファイルへと変換したデータセットを再び個々の画像ファイルに復元し、従来と同様の方法で学習処理を行います。\n"
     ]
    }
   ],
   "source": [
    "response2 = query_engine2.query(\"提案手法の概要を説明してください\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c0b66-c3c5-4164-a0a5-db4cd625d231",
   "metadata": {},
   "source": [
    "# ベクターストアから復元(成功？)\n",
    "一応，embeddingは質問の1回だけしか行われなかった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96c4d51d-30fc-4420-99b3-4b88ac798776",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2.storage_context.persist(persist_dir=\"./storage_context\")\n",
    "# documentもqdrantに保存されていて，出力されるjsonはそのアドレスを示しているだけ？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98763aac-673d-4e93-8a83-089dc56b93a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "\n",
    "storage_context3 = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"./storage_context\"),\n",
    "    vector_store=vector_store2,\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"./storage_context\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91077b46-335f-4d6a-a0fb-ceb035953c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "index3 = load_index_from_storage(storage_context3, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43073b40-1aab-4e88-bba2-cdacf05d3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine3 = index3.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d6b1380-02fa-4d2f-b54e-f53fb779965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://qdrant:6333/collections/test_rag2/points/search \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://qdrant:6333/collections/test_rag2/points/search \"HTTP/1.1 200 OK\"\n",
      "画像加工による異常検知モデル学習用データセットの圧縮手法の提案では、以下の手順でデータセットの圧縮を行っています。\n",
      "\n",
      "まず、各画像の切り抜き領域を示すマスク画像の論理和を求め、すべてのオブジェクトが収まる最小領域を持ったマスク画像を作成します。これにより、オブジェクトに干渉しない範囲の背景を大まかに削除することができます。\n",
      "\n",
      "次に、加工した画像データセットを動画ファイルに変換します。変換は、HEVCの非可逆圧縮モードを用いて行われ、画像1枚を動画データの1フレームとして圧縮します。HEVCによる変換では、画像間の冗長性を排除して符号化されるため、画像データセットのサイズが大幅に縮小されることが期待されます。\n",
      "\n",
      "最後に、モデルの学習時には、一度動画ファイルへと変換したデータセットを再び個々の画像ファイルに復元します。これにより、従来と同様の方法で学習処理を行うことができます。\n"
     ]
    }
   ],
   "source": [
    "response3 = query_engine3.query(\"提案手法の概要を説明してください\")\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d327ca-97e8-4401-add8-8637bef83e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
