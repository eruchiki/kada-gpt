{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e7abcb-eff5-46a9-b77f-e85a7141a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4acf28cb-5ffc-4a7e-bc36-c643f1f95cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PDFMinerLoader\n",
    "\n",
    "loader = PDFMinerLoader(\"data/H-001.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"./storage\")\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90d1267-af0b-4058-acd0-c7e405f70f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='H-001\\n\\n 123\\n第 3 分冊', metadata={'source': 'data/H-001.pdf'}), Document(page_content='顔画像を用いた個人特徴の減算によるユーザの曖昧な内部状態推定AmbiguousUserStatesEstimationbySubtractionofIndividualFeaturesfromFacialImages朝枝彩夏1）武村紀子1）AyakaAsaedaNorikoTakemura1はじめに*新型コロナウイルス感染症をきっかけとして，教育現場では自宅で学習が行えるe-Learningが利用される機会が増加している．また，仕事現場においてはオフィスに出社せず自宅から遠隔で業務が行える在宅勤務を導入する企業が増加している．しかし，オンラインでの作業は，長時間の着座や動画の視聴による疲労や眠気，また，周りに人がおらず緊張感がなくなることによる集中力の低下などを招く恐れがある．こうした事態を未然に防ぐために，近年，ユーザの状態を推定し注意を促すようなシステムが注目を集めている．このような', metadata={'source': 'data/H-001.pdf'}), Document(page_content='システムの開発のためには，ユーザの正確な内部状態の把握が求められる．内部状態の推定には表情や心拍，脳活動などさまざまなセンシングデータが用いられている．その中でも，表情は内部状態が表れやすく，データの収集も容易である点から，数多くの状態推定の研究に用いられてきた[1][2]．しかし，この表情を用いて状態推定を行う表情認識には問題点がある．人の顔に共通して備わる眉，目，鼻，口といったパーツは，大きさや間隔，角度，形などは人によって異なる．そのような顔の構造における個人差は個人識別を行う上では不可欠な要素となる一方，表情認識を行う上では精度に悪影響を及ぼす可能性がある．また，顔の構造の他にも文化的差異によって表情表出の仕方やその強弱などに個人差が生じることが知られている[3][4][5]．このような個人差の問題を解決するために考えられる方法として，年齢や性別，文化の異なるなるべく多くの人の顔画', metadata={'source': 'data/H-001.pdf'}), Document(page_content='像データを収集することが挙げられる．しかし，顔画像には個人を特定するための要素が数多く含まれており，プライバシーの問題から多くのデータを収集することは容易ではない．また，機械学習による状態推定モデルを構築するためには，各データに対して内部状態のラベルが必要となるが，アノテーションにかかる時間的，金銭的コストは大きく，データセットが大規模になるほどそのコストも比例して大きくなってしまう．そのため，本研究では少量のデータセットで個人差を考慮した表情認識の手法の開発を目指す．個人差を考慮した表情認識の研究は既にいくつか行われている．Xieら[6]はある顔画像から抽出した表情を任意の顔画像に埋め込むことで新たな表情画像を生成するTwo-branchDisentangledGenerativeAdversarialNetworkを用いて，個人差を考慮した表情認識を行った．またLiuら[7]は特徴空間', metadata={'source': 'data/H-001.pdf'}), Document(page_content='において異なる被写体の同一表情の距離が同一の被写体の異なる表情の距離よりも小さくなるように学習を行うことで，個人差を考慮した表情認識を行った．他にも3D顔モデルを生成して表情特徴を取り出す手法やCNNにおいて個人特徴と表情1）九州工業大学KyushuInstituteofTechnology特徴を分離する手法など表情認識における個人差の影響を抑えるために様々な試みが行われてきた[8][9][10]．これらの従来研究の多くは比較的推定が容易な基本感情を推定の対象としている．しかし，実システムへの応用を考えると，単純で明確な感情の推定だけではなく，集中や眠気，疲労といった曖昧な内部状態の推定が重要となる．そこで本研究では個人差を考慮した表情認識の中でも，曖昧な内部状態を対象とした状態推定を目指す．曖昧な内部状態は表情に表れにくく，変化も微小であるため，基本感情の推定よりも個人差による影響が大き', metadata={'source': 'data/H-001.pdf'}), Document(page_content='い．例えば，覚醒度の推定であれば，眠っている状態と起きている状態は単純に瞼の開閉動作を見れば状態の判別は容易であるが，眠たい状態は瞼だけではなく，眉の角度や口の開き具合など全体的な様子から微妙な変化を読み取らなければならないため判別が難しい．本研究では，現存の基本感情を推定の対象とした表情認識手法で高い精度が得られているDeviationLearningNetwork(以下，DLN)[10]に基づいて，曖昧な内部状態推定を行う．具体的には，DLNで用いられている偏差モジュールを用いて，顔画像特徴から個人特徴を減算することで個人によらない顔画像特徴を抽出し，内部状態の推定を行う．評価実験では，e-Learning時の学習者の顔画像データを用いて，学習者の3段階の覚醒度（Awake/Drowsy/Asleep）を推定し，本手法の有用性を検証する．2提案手法人物が映った画像から顔領域を抽出し，顔', metadata={'source': 'data/H-001.pdf'}), Document(page_content='画像をDLNに基づく状態推定モデルに入力することで内部状態の推定を行う．以下に各手法の詳細を述べる．2.1顔検出手法Multi-taskCascadedConvolutionalNeuralNet-works(以下，MTCNN)[11]を用いて画像データ中の学習者の顔領域のみを160×160で抽出し，顔画像データを作成した．MTCNNとは，顔領域の検出を行うProposalNetwork(P-Net)，P-Netの出力をもとに顔でない領域部分を候補から削除するRefineNetwork(R-Net)，R-Netの出力をもとに目・鼻・口部分を検出し，最終的に顔領域を出力するOutputNetwork(O-Net)の3段階のCNNから構成される顔検出手法である．顔の最小検知サイズは90に設定し，学習者の背後に他の人が映っていても学習者以外は顔検知の対象から外れるよう処理を行った．なお，MTCN', metadata={'source': 'data/H-001.pdf'}), Document(page_content='Nは一定の角度以上学習者が下を向いている場合は顔検知を行うことができない．画像データをMTCNNに適用した例を図1に示す．*本論文は画像の認識・理解シンポジウムMIRU2023においてコンセプト論文として発表した内容に基づくFIT2023（第22回情報科学技術フォーラム）Copyright', metadata={'source': 'data/H-001.pdf'}), Document(page_content='© 2023 byThe Institute of Electronics, Information and Communication Engineers andInformation Processing Society of Japan All rights reserved.\\x0c 124', metadata={'source': 'data/H-001.pdf'}), Document(page_content='第 3 分冊', metadata={'source': 'data/H-001.pdf'}), Document(page_content='切り取り前切り取り後図1:MTCNN適用例図2:提案手法の概要2.2状態推定手法状態推定手法の概要を図2に示す．まず，Googleが開発したFaceNet[12]と呼ばれる顔認証用のモデルを用いて個人識別モデルを学習する．FaceNetとは，顔画像から抽出された特徴をユークリッド空間へ最適な埋め込みができるよう学習を行い，生成した空間での顔画像間の距離を計算することで，顔の類似度を求めることができる手法である．個人識別モデルの学習には，様々な年齢や人種を含む9,131人の顔画像データセット（約331万枚）であるVggFace2[13]を用い，FaceNetを構成するCNNとしてはInceptionResnet(v1)[14]を用いる．次に，個人識別モデルと全く同じ構造，同じ重みの顔認識モデルを用意し，並列ネットワークを構築する．本ネットワークを学習させる際は，個人識別モデルの重みは固定し', metadata={'source': 'data/H-001.pdf'}), Document(page_content='，顔認識モデルのみを学習させる．顔認識モデルから出力される顔画像特徴𝑉face（512次元）から個人識別モデルから出力される個人特徴𝑉id（512次元）を要素ごとに減算することで，個人によらない512次元の顔画像特徴𝑉stateを得る．最後に，全結合層からなる状態推定モジュールにおいて内部状態の推定を行う．最終層の出力（𝑁𝑐次元，𝑁𝑐は識別状態クラス数）にソフトマックス関数を適用し，損失関数は交差エントロピー誤差を用いる．また各層において，活性化関数RectifiedLinearUnitおよびDropout（選出率0.4）を適用する．3評価実験提案手法の有効性を検証するために，講義動画視聴時の学習者の顔画像データを用いて覚醒度の推定を行う．3.1データセット大学生53名の被験者に対し，講義動画（スライド＋音声，情報学に関する内容）を視聴している様子をカメラで撮影した．講義動画の視聴はノート', metadata={'source': 'data/H-001.pdf'}), Document(page_content='パソコンを用いて行うものとし，ノートパソコン内蔵のカメラを表1:アノテーション基準ラベル基準Asleep-1秒以上目を閉じている-まぶたが常に開いている状態でないDrowsy-瞳孔が動かない-目を閉じている時間が1秒以内である-体や頭の動きが制御できていない-まぶたを大きく開けている状態Awake-瞳孔が左右に動く-体や頭の動きが制御できている図3:ラベルの内訳用いて図1左のような上半身の画像を撮影する．データ収集実験は4回に分けて行い，被験者は1回の実験あたり講義動画（約10分）を1〜3本視聴する．ただし，各被験者が参加可能な回のみデータ収集を行ったため，被験者により総データ数は異なる．撮影した画像サイズは640×480画素で，フレームレートは30fpsである．また，1秒ごとに覚醒度のラベル（Asleep（眠っている），Drowsy（眠そう），Awake（起きている））を付与している．覚', metadata={'source': 'data/H-001.pdf'}), Document(page_content='醒度のアノテーションはアノテータ間での一貫性を持たせるため，表1に示すアノテーションの基準を設けている．ここでは，すべてのラベル(Asleep/Drowsy/Awake)についてデータが存在する被験者27名について評価実験を行う．各被験者のデータにおけるラベルの内訳を図3に示す．3.2比較手法個人差を考慮した状態推定モデルである提案手法の有効性を検証するため，偏差モジュールの代わりに顔認識モデルのみで覚醒度の推定を行う手法を用いて比較実験を行う．ただし，比較手法を学習する際の顔認識モデルの重みの初期値は提案手法と同じものを用いる．比較手法の概要を図4に示す．また，本実験では状態推定モジュールにおける全結合層の数を1層（512→3次元），2層（512→128→3次元），3層（512→128→32→3次元）とする3つのパターンについて推定精度の比較を行った．3.3評価手法27名分の被験者を3名', metadata={'source': 'data/H-001.pdf'}), Document(page_content='ずつ，9グループに分割し，Leaveone-groupout交差検証を行う．1グループをテストデータ，他の1グループを検証データ，残りの7グループを学習データとし，すべてのグループが1回ずつFIT2023（第22回情報科学技術フォーラム）Copyright', metadata={'source': 'data/H-001.pdf'}), Document(page_content='© 2023 byThe Institute of Electronics, Information and Communication Engineers andInformation Processing Society of Japan All rights reserved.\\x0c 125', metadata={'source': 'data/H-001.pdf'}), Document(page_content='第 3 分冊', metadata={'source': 'data/H-001.pdf'}), Document(page_content='図4:比較手法の概要表2:各グループのデータAsleepDrowsyAwakegroup1543054275381group2186301944619698group3127521788119405group4461524958349587group566211042610448group6144571424414330group7128101589716033group8271142758326976group9840083248390テストデータになるように計9回実験を行い，9回分の評価値の平均により性能評価を行う．表2に各グループにおける各ラベルの内訳を示す．なお，データに対しては被験者ごとにアンダーサンプリングを行っている．評価指標には多クラス分類においてクラスごとに計算したF1scoreの平均を取るmacro-F1scoreを用いる．以下，本実験で使用したハイパーパラメータの設', metadata={'source': 'data/H-001.pdf'}), Document(page_content='定について述べる．本実験ではエポック数は3に設定した．また，ミニバッチ学習を用いており，ミニバッチサイズは128に設定した．学習率の初期値は0.001に設定し，1エポック目と2エポック目が終わった段階で学習率を0.1ずつ乗算するように設定した．加えて，過学習抑制の手法のひとつであるweightdecayを導入し，値は0.001と設定した．本実験では100バッチごとに検証データを用いてモデルの性能を測り，その中でmacro-F1scoreの値が最も高かったモデルを用いて，テストデータによる評価を行った．OptimizerにはStochasticGradientDescent(SGD)を用いた．3.4実験結果表3に提案手法及び比較手法におけるmacro-F1scoreの結果を示す．提案手法と比較手法のmacro-F1scoreを比較すると，すべてのパターンで提案手法が比較手法の精度を上回った．', metadata={'source': 'data/H-001.pdf'}), Document(page_content='このことから，偏差モジュールにより顔画像特徴から個人の特徴を減算し，個人によらない顔画像特徴を抽出することが曖昧な内部状態推定に有効であることがわかる．状態推定モジュールの層の数についての考察状態推定モジュールにおける層の数による精度の違いについて比較する．図5に学習及び検証データにおける学習曲線を示す．各次元削減パターンを比較すると，提案手法と比較手法ともに512次元から128次元に削減した後，3次元に削減する2層の場合が最も高い値を示した．一般に，ニューラルネットワークは層を深くすることでパラメータ数が多くなり，モデルの表現力が上がる表3:提案手法及び比較手法のmacro-F1score状態推定モジュール提案手法比較手法512→30.5260.487512→128→30.5550.505512→128→32→30.5090.474状態推定モジュール：1層状態推定モジュール：2層状態推定', metadata={'source': 'data/H-001.pdf'}), Document(page_content='モジュール：3層図5:学習曲線（状態推定モジュールの層数を変化）ことが知られている．しかしパラメータ数が多いほど，最適化を行う際の探索空間は複雑になり，学習が難しくなる傾向にある．層の数が1層，2層，3層における学習曲線をそれぞれ比較すると，学習時における精度推移は1層や2層の場合は90％近くで収束しているのに対して，3層の場合では70％程と低い傾向にあった．このことから，3層は上手く学習を行うことができず，結果1層よりもパラメータ数が多く表現力のある2層が最も精度の良いモデルとなったと考えられる．予測値の内訳についての考察状態推定モジュールを2層にした場合における提案手法の混合行列を表4に示す．ただし，本混合行列は9回分の交差検証のテストデータの結果を全て足し合わせたものである．正解ラベルと予測ラベルが一致している箇所を見ると，AsleepとAwakeは比較的正しく推定できているのに対し', metadata={'source': 'data/H-001.pdf'}), Document(page_content='，Drowsyはあまり正しく推定できていないことがわかる．さらに詳細な内訳を見ると，“Asleep/Awake”および“Asleep/Drowsy”は高い精度で識別できているが，“Drowsy/Awake”の識別を失敗しているケースが多い．これは，Asleepは完全に瞼を閉じた状態であるのに対し，DrowsyおよびAwakeはどちらも瞼は開いた状態であり，微妙な瞼の開き具合や眉の角度，口形の微妙な違いに基づいて判断しなければならないためである．また，このことは同時にアノテーションの難しさにも繋がる．Drowsyについてはアノテータにより判断が異なるケースが存在し，このアノテーションの曖昧さが推定精度の低下をもたらしている可能性も考えられる．FIT2023（第22回情報科学技術フォーラム）Copyright', metadata={'source': 'data/H-001.pdf'}), Document(page_content='© 2023 byThe Institute of Electronics, Information and Communication Engineers andInformation Processing Society of Japan All rights reserved.\\x0c 126', metadata={'source': 'data/H-001.pdf'}), Document(page_content='第 3 分冊', metadata={'source': 'data/H-001.pdf'}), Document(page_content='表4:提案手法の混同行列正解ラベルAsleepDrowsyAwakeAsleep1104992277810972予測ラベルDrowsy235126390756070Awake1835582126103206計1523661688111702484終わりに表情認識において，個人の顔の作りや表情の表出方法の違いが認識精度に影響を及ぼすことが問題として挙げられている．そこで，本研究では個人差の影響を低減させる偏差モジュールを用いて，基本感情よりも推定が難しい曖昧な内部状態の推定を行う手法を提案した．e-Learning中の学習者の顔画像データを用いて覚醒度推定問題について評価実験を行った結果，提案手法の有効性を示すことができた．しかしながら，問題点として眠そうな状態と起きている状態を正確に識別することが難しい点が挙げられる．これは，眠そうな状態と起きている状態の判別が難しく，アノテーションに曖', metadata={'source': 'data/H-001.pdf'}), Document(page_content='昧性があることも原因の一つであると考えられる．そこで今後の展望として，曖昧なアノテーションをソフトラベルとして扱うなど，アノテーションの曖昧性の問題に対処していく．また，現在は状態推定の際に1枚の画像のみを入力としているが，動きの情報は曖昧な状態を推定する上で有用であると考えられるため，時系列データを入力できるように本手法を拡張する．参考文献[1]Ji-HaeKim,Byung-GyuKim,ParthaPratimRoy,andDa-MiJeong.Efficientfacialexpressionrecognitionalgorithmbasedonhierarchicaldeepneuralnetworkstructure.IEEEAccess,Vol.7,pp.41273–41285,2019.[2]HongliZhang,AlirezaJolfaei,andMamounAlaza', metadata={'source': 'data/H-001.pdf'}), Document(page_content='b.Afaceemotionrecognitionmethodusingconvolutionalneu-ralnetworkandimageedgecomputing.IEEEAccess,Vol.7,pp.159081–159089,2019.[3]HillaryElfenbein,MartinBeauprÃ©,ManonLÃ©vesque,andUrsulaHess.Towardadialecttheory:Culturaldiffer-encesintheexpressionandrecognitionofposedfacialex-pressions.Emotion(Washington,D.C.),Vol.7,pp.131–46,032007.[4]WallaceV.Friesen.Culturaldifferencesinfacialexpres-sionsinasocial', metadata={'source': 'data/H-001.pdf'}), Document(page_content='situation:Anexperimentaltestonthecon-ceptofdisplayrules.1973.[5]PaulaNiedenthal,MagdalenaRychlowska,FangyunOliviaZhao,andAdrienneWood.Historicalmigrationpatternsshapecontemporaryculturesofemotion.Perspectivesonpsychologicalscience:ajournaloftheAssociationforPsy-chologicalScience,Vol.14,p.1745691619849591,062019.[6]SiyueXie,HaifengHu,andYizhenChen.Facialexpres-sionrecognitionwithtwo-branchdisentang', metadata={'source': 'data/H-001.pdf'}), Document(page_content='ledgenerativeadversarialnetwork.IEEETransactionsonCircuitsandSys-temsforVideoTechnology,Vol.31,No.6,pp.2359–2371,2021.[7]XiaofengLiu,B.V.K.VijayaKumar,PingJia,andJaneYou.Hardnegativegenerationforidentity-disentangledfacialexpressionrecognition.PatternRecogn.,Vol.88,No.C,pp.1–12,apr2019.[8]ZiboMeng,PingLiu,JieCai,ShizhongHan,andYanTong.Identity-awareconvolutionalneuralnetworkforfacialex-pressionrec', metadata={'source': 'data/H-001.pdf'}), Document(page_content='ognition.In201712thIEEEInternationalCon-ferenceonAutomaticFace&GestureRecognition(FG2017),pp.558–565,2017.[9]MohammadRamiKoujan,LumaAlharbawee,GiorgosGi-annakakis,NicolasPugeault,andAnastasiosRoussos.Real-timefacialexpressionrecognition“inthewild”bydisentan-gling3dexpressionfromidentity.In202015thIEEEInter-nationalConferenceonAutomaticFaceandGestureRecog-nition(FG2020),pp.24–31,2020.[10]WeiZhang,X', metadata={'source': 'data/H-001.pdf'}), Document(page_content='ianpengJi,KeyuChen,YuDing,andChangjieFan.Learningafacialexpressionembeddingdisentan-gledfromidentity.In2021IEEE/CVFConferenceonCom-puterVisionandPatternRecognition(CVPR),pp.6755–6764,2021.[11]KaipengZhang,ZhanpengZhang,ZhifengLi,andYuQiao.Jointfacedetectionandalignmentusingmultitaskcas-cadedconvolutionalnetworks.IEEESignalProcessingLet-ters,Vol.23,No.10,pp.1499–1503,2016.[12]FlorianSchroff,DmitryK', metadata={'source': 'data/H-001.pdf'}), Document(page_content='alenichenko,andJamesPhilbin.Facenet:Aunifiedembeddingforfacerecognitionandclus-tering.In2015IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pp.815–823,2015.[13]QiongCao,LiShen,WeidiXie,OmkarM.Parkhi,andAn-drewZisserman.Vggface2:Adatasetforrecognisingfacesacrossposeandage.In201813thIEEEInternationalConfer-enceonAutomaticFaceandGestureRecognition(FG2018),pp.67–74,2018.[14]ChristianSzegedy,', metadata={'source': 'data/H-001.pdf'}), Document(page_content='SergeyIoffe,VincentVanhoucke,andAlexanderA.Alemi.Inception-v4,inception-resnetandtheimpactofresidualconnectionsonlearning.InProceedingsoftheThirty-FirstAAAIConferenceonArtificialIntelligence,AAAI’17,pp.4278–4284.AAAIPress,2017.FIT2023（第22回情報科学技術フォーラム）Copyright', metadata={'source': 'data/H-001.pdf'}), Document(page_content='© 2023 byThe Institute of Electronics, Information and Communication Engineers andInformation Processing Society of Japan All rights reserved.', metadata={'source': 'data/H-001.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68cbd172-5a5e-424d-ad2a-3477264e391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: この論文を要約して。\n",
      "A:  この論文では、顔画像を用いた個人特徴の減算によるユーザの曖昧な内部状態推定を提案しています。偏差モジュールを使用して、個人の特徴を顔画像特徴から減算し、個人によらない顔画像特徴を抽出します。状態推定モジュールの層の数を比較すると、512次元から128次元に削減した後、3次元に削減する2層の場合が最も高い値を示しました。今後の展望として、曖昧なアノテーションをソフトラベルとして扱うなど、アノテーションの曖昧性の問題に対処していくこと、また、時系列データを入力できるように本手法を拡張することを提案しています。\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma(persist_directory=\"./storage\", embedding_function=embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0, max_tokens=500, streaming=True)\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# import sys\n",
    "# args = sys.argv\n",
    "# # if len(args) >= 2:\n",
    "# #  query = args[1]\n",
    "# # else:\n",
    "# query = \"この論文を要約して。\"\n",
    "# answer = qa.run(query)\n",
    "\n",
    "# print(\"Q:\", query)\n",
    "# print(\"A:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5102bbe-6675-471d-8496-aaf523c96b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "聞きたいことを教えてください。\n",
      ">>> この論文を要約して\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: この論文を要約して\n",
      "A:  本論文では、顔画像から個人の特徴を減算し、個人によらない顔画像特徴を抽出することにより、ユーザの曖昧な内部状態を推定する手法を提案した。実験結果から、提案手法は比較手法の精度を上回ったことがわかった。また、状態推定モジュールの層の数についても検証し、2層が最も高い精度を示した。今後の展望として、曖昧なアノテーションをソフトラベルとして扱うなど、アノテーションの曖昧性の問題に対処し、時系列データを入力できるように本手法を拡張することを提案した。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(llm\u001b[38;5;241m=\u001b[39mllm, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mretriever, chain_type_kwargs\u001b[38;5;241m=\u001b[39mchain_type_kwargs)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m聞きたいことを教えてください。\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m>>>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     query \u001b[38;5;241m=\u001b[39m inp\n\u001b[1;32m     33\u001b[0m     answer \u001b[38;5;241m=\u001b[39m qa\u001b[38;5;241m.\u001b[39mrun(query)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# 会話履歴を参照する\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer in Japanese:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma(persist_directory=\"./storage\", embedding_function=embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0, max_tokens=500, streaming=True)\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs=chain_type_kwargs, memory=memory)\n",
    "\n",
    "while True:\n",
    "    inp = input(\"Q: \")\n",
    "    query = inp\n",
    "    answer = qa.run(query)\n",
    "    print(\"A:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cda53d-03d0-4b24-9eab-e234f7016754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  要約して\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study compares the accuracy of state estimation modules with different numbers of layers. It is found that reducing the dimensions from 512 to 128 and then to 3 in a 2-layer module yields the highest accuracy. Increasing the number of layers to 3 does not improve the learning and results in lower accuracy compared to 1 or 2 layers. The study also discusses the impact of parameter complexity on learning difficulty.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "import chromadb\n",
    "\n",
    "from langchain.document_loaders import OnlinePDFLoader, UnstructuredPDFLoader, PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "loader = PyPDFLoader(\"data/H-001.pdf\")\n",
    "pdfData = loader.load()\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splitData = text_splitter.split_documents(pdfData)\n",
    "collection_name = \"clarett_collection\"\n",
    "local_directory = \"clarett_vect_embedding\"\n",
    "persist_directory = os.path.join(os.getcwd(), local_directory)\n",
    "\n",
    "openai_key=os.environ.get('OPENAI_API_KEY')\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_key)\n",
    "vectDB = Chroma.from_documents(splitData,\n",
    "                      embeddings,\n",
    "                      collection_name=collection_name,\n",
    "                      persist_directory=persist_directory\n",
    "                      )\n",
    "vectDB.persist()\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chatQA = ConversationalRetrievalChain.from_llm(\n",
    "            OpenAI(openai_api_key=openai_key,\n",
    "               temperature=0, model_name=\"gpt-3.5-turbo\"), \n",
    "            vectDB.as_retriever(), \n",
    "            memory=memory)\n",
    "\n",
    "chat_history = []\n",
    "qry = \"\"\n",
    "while qry != 'done':\n",
    "    qry = input('Question: ')\n",
    "    if qry != exit:\n",
    "        response = chatQA({\"question\": qry, \"chat_history\": chat_history})\n",
    "        print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb70113-6798-46b2-bcea-cf08fd36fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = \"\"\"\n",
    "# Below is a history of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base **about employee healthcare plans and the employee handbook**.\n",
    "# **Generate a search query based on the conversation and the new question**. \n",
    "# Do not include cited source filenames and document names e.g info.txt or doc.pdf in the search query terms.\n",
    "# Do not include any text inside [] or <<>> in the search query terms.\n",
    "# If the question is not in English, translate the question to English before generating the search query.\n",
    "\n",
    "# **Chat History:\n",
    "# {chat_history}**\n",
    "\n",
    "# **Question:\n",
    "# {question}**\n",
    "\n",
    "# Search query:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"chat_history\", \"question\"],\n",
    "#     template=template,\n",
    "# )\n",
    "# prompt.format(product=\"colorful socks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc9cea-a4b8-41f2-9c38-2502f6978f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentを利用する\n",
    "template_prefix = \\\n",
    "\"You are an intelligent assistant helping Contoso Inc employees with their healthcare plan questions and employee handbook questions. \" \\\n",
    "\"Answer the question using only the data provided in the information sources below. \" \\\n",
    "\"For tabular information return it as an html table. Do not return markdown format. \" \\\n",
    "\"Each source has a name followed by colon and the actual data, quote the source name for each piece of data you use in the response. \" \\\n",
    "\"For example, if the question is \\\"What color is the sky?\\\" and one of the information sources says \\\"info123: the sky is blue whenever it's not cloudy\\\", then answer with \\\"The sky is blue [info123]\\\" \" \\\n",
    "\"It's important to strictly follow the format where the name of the source is in square brackets at the end of the sentence, and only up to the prefix before the colon (\\\":\\\"). \" \\\n",
    "\"If there are multiple sources, cite each one in their own square brackets. For example, use \\\"[info343][ref-76]\\\" and not \\\"[info343,ref-76]\\\". \" \\\n",
    "\"Never quote tool names as sources.\" \\\n",
    "\"If you cannot answer using the sources below, say that you don't know. \" \\\n",
    "\"\\n\\nYou can access to the following tools:\"\n",
    "\n",
    "template_suffix = \"\"\"\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Thought: {agent_scratchpad}\"\"\"\n",
    "\n",
    "acs_tool = Tool(name=\"CognitiveSearch\", \n",
    "                    func=lambda q: self.retrieve(q, overrides), \n",
    "                    description=self.CognitiveSearchToolDescription,\n",
    "                    callbacks=cb_manager)\n",
    "employee_tool = EmployeeInfoTool(\"Employee1\", callbacks=cb_manager)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
