{"prompt": "System: あなたは人間と友好的に会話するAIです。
AIは以下に示される番号付きのドキュメント情報を使って、質問に対して詳細かつ正確な回答を提供します。
AIは回答に使用したドキュメントの番号を、該当する箇所に引用の形式で示します。
引用例：これは引用です[0-1]。これも引用です[2-5]。これは複数の引用を使っています[4-0][2-2]。この文書には引用が必要ありません。
AIはドキュメント情報を使って質問に回答できない場合、「わかりません」と回答します。
ドキュメントはデータベースに保存されており、ドキュメント情報グループの先頭にはそのファイル名とグループ番号が与えられるので考慮に入れてください。
グループ内のドキュメント情報は文章の塊ごとに改行で区切られています。情報は塊同士で分けて考えてください。

# ドキュメント情報
- [0] pdf_files/1/20240410081541755204_20T331_増田_final.pdf
  - [0-0] - 1 - コスト削減手法を取り入れたRAG による質問応答システムの提案 Proposal of a Question-Answering System Using RAG with Cost Reduction Techniques  増田 嶺*1 岩本 和真*1 道信 祐成*1  北 健志*2 竹原 一駿*1  Rei Masuda Kazuma Iwamoto Yusei Michinobu Kenji Kita Ichitoshi Takehara  安藤 一秋*1 亀井 仁志*1 最所 圭三*1  喜田 弘司*1  Kazuaki Ando Hitoshi Kamei Keizo Saisho Koji Kida  *1 香川大学 *2 株式会社STNet  Kagawa University STNet, Inc. Retrieval-Augmented Generation (RAG) is a technique that enables question-answering for internal organizational documents by integrating external information with large-scale language models. In recent years, there has been a growing trend in question-answering services that combine ChatGPT with RAG. However, using high-performance models like GPT-4 in large-scale settings can lead to increased API costs due to the rising number of input tokens. This study proposes an additional step that utilizes lower-cost models, such as GPT-3.5, to selectively extract only the necessary information from documents before generating responses. This approach aims to reduce the number of tokens used during response generation, thereby potentially lowering the operational costs associated with GPT-4. The paper also compares the results of this proposed method with those of conventional methods to assess its effectiveness. The findings indicate that the proposed method manages to reduce costs while maintaining accuracy.  1.

  - [0-1] 文書選択のための圧縮機では,デュアルエンコーダを用いて,質問文とドキュメントの2 つの文章間の類似度を学習する.このアプローチは,RAG と似た概念であるIn-Context RALM[Ram 2023]で応答性能を高め,計算コストを削減することを示している. 2.2 応答出力の自己評価による品質向上 RAG において,LLM が出力した文章の品質と正確性を高める手法として,Self-RAG[Asai 2023]が提案されている.このフレームワークでは,過去の質問応答と新しい質問に対する生成中の応答が外部情報を必要とするか,LLM 自身で評価・判断する.モデル推論時には,クエリやドキュメントに対してその関係性を表すReflection トークンと呼ばれるタグを用い,回答に利用する生成文を選択する.Self-RAG は,従来のRAG と比較して情報の正確性や応答性能が向上すると示されている. 2.3 生成型と抽出型リーダに関する調査 Luo らによる調査[Luo 2022]では,トランスフォーマーベースの事前学習済みLLM において,生成型リーダと抽出型リーダを比較している.生成型リーダは与えられた質問とドキュメントから応答を生成する.

  - [0-2] はじめに 近年,OpenAI社のChatGPTを始めとする大規模言語モデル(以下,LLM)の活用が期待されており,質問応答,文書生成,要約,翻訳など,様々なタスクへの適用が進んでいる. 質問応答タスクのうち,組織内のPDF 文書や新聞記事の内容など,学習済みのLLM が知り得ない外部情報を回答させる手法として,Retrieval-Augmented Generation[Lewis 2020] (以下,RAG)が提案されている.RAG は,検索した外部情報を含む文書(以下,ドキュメント)をプロンプトに組み込むことで質問に関する情報を正確に回答させることが可能であり,事実に基づかない回答を生成するHallucination を解決できる. また,RAG の応答生成では,API を通じて外部LLM が利用されることも多い. OpenAI API を用いたRAG の場合,入出力トークン数に応じて利用料金が発生する仕組みのため,入力トークン数が増加するほど,API の利用料金も増大する.従来のRAG は固定長のドキュメントをそのまま応答生成に利用するため,不必要な情報が含まれることとなり,必要数以上のトークンが用いられる. 我々の研究では,一般的なRAG に,安価なGPT-3.

  - [0-3] 対して抽出型リーダは,与えられた質問を参考に,ドキュメントから回答を含む箇所を抽出する.結果として,生成型リーダは長いコンテキストを含む質問応答で高いパフォーマンスを発揮し,抽出型リーダは短いコンテキストで高いパフォーマンスを発揮することが示され,外部情報を利用する際にも優れていることが述べられている.我々の研究では,ドキュメント毎の選択処理を行った後に応答生成処理を行うことで,質問応答性能を維持する狙いがある. 連絡先: 増田嶺 香川大学 創造工学部 〒761-0396 香川県高松市林町2217−20 電話番号: 087-864-2291  - 2 - 3. 提案手法 3.1 概要 提案手法のプロセスの概要を図 1 に示す.本研究では,従来のRAG における,ドキュメントを検索し回答を生成するというプロセスの間に,関連情報の選択を行う処理を追加する手法を提案する.ここでドキュメントの精査を行い,回答に必要な情報を含む文章のみを抜き出す. ドキュメントの検索には,既存の手法を用いる.


Human: RAGとはどのような手法ですか？", "answer": "RAG（Retrieval-Augmented Generation）は、検索した外部情報を含む文書（ドキュメント）をプロンプトに組み込むことで、質問に関する情報を正確に回答させる手法です。この手法は、事実に基づかない回答を生成する問題（Hallucination）を解決できるとされています[0-2]。", "cost": 0.0, "res_line": [{"number": "0-2", "sentence": "はじめに 近年,OpenAI社のChatGPTを始めとする大規模言語モデル(以下,LLM)の活用が期待されており,質問応答,文書生成,要約,翻訳など,様々なタスクへの適用が進んでいる. 質問応答タスクのうち,組織内のPDF 文書や新聞記事の内容など,学習済みのLLM が知り得ない外部情報を回答させる手法として,Retrieval-Augmented Generation[Lewis 2020] (以下,RAG)が提案されている.RAG は,検索した外部情報を含む文書(以下,ドキュメント)をプロンプトに組み込むことで質問に関する情報を正確に回答させることが可能であり,事実に基づかない回答を生成するHallucination を解決できる. また,RAG の応答生成では,API を通じて外部LLM が利用されることも多い. OpenAI API を用いたRAG の場合,入出力トークン数に応じて利用料金が発生する仕組みのため,入力トークン数が増加するほど,API の利用料金も増大する.従来のRAG は固定長のドキュメントをそのまま応答生成に利用するため,不必要な情報が含まれることとなり,必要数以上のトークンが用いられる. 我々の研究では,一般的なRAG に,安価なGPT-3."}], "res_file": [{"number": 0, "file_name": "pdf_files/1/20240410081541755204_20T331_増田_final.pdf"}]}